{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3OeUnmq7F4L",
        "outputId": "ddfb42d1-c9c1-4c59-cad9-a52a848a1b52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Uip9C-n7Lo-",
        "outputId": "87a2b8c9-b7f4-43f8-d483-eb37041eae5c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.12/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.12/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')  # For lemmatization with WordNet\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKqLwZPR7Un5",
        "outputId": "b1b0522d-52e9-4a9f-c015-b90360510ac4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the file\n",
        "with open('file.txt', 'w') as f:\n",
        "    f.write(\"Thiss is a samplle textt corpus for NLP pre processing. It containss spellling errorrs and runn-onn sentencess. The weather is nicce todayy, isnt it? We went to the park but it rainned heavilly. Machinne learning is funn but challengging.\")\n",
        "\n",
        "# Verify (run in next cell)\n",
        "with open('file.txt', 'r') as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hte_-dYF7XV5",
        "outputId": "1c8ee78a-41d3-45b0-b10f-79d2198d4673"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thiss is a samplle textt corpus for NLP pre processing. It containss spellling errorrs and runn-onn sentencess. The weather is nicce todayy, isnt it? We went to the park but it rainned heavilly. Machinne learning is funn but challengging.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from textblob import TextBlob  # For spelling correction\n",
        "import re  # For basic cleaning if needed\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "NvJwqcV87e7i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the text corpus\n",
        "with open('file.txt', 'r', encoding='utf-8') as f:\n",
        "    raw_text = f.read().strip()\n",
        "\n",
        "print(\"Raw text corpus:\")\n",
        "print(raw_text)\n",
        "print(\"\\n\" + \"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23-Oyu6m7g_C",
        "outputId": "2d46bf40-fa59-4dbd-8289-0c05a31c7988"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw text corpus:\n",
            "Thiss is a samplle textt corpus for NLP pre processing. It containss spellling errorrs and runn-onn sentencess. The weather is nicce todayy, isnt it? We went to the park but it rainned heavilly. Machinne learning is funn but challengging.\n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the raw text\n",
        "tokens = word_tokenize(raw_text.lower())  # Convert to lowercase for consistency\n",
        "\n",
        "print(\"First 30 tokens:\")\n",
        "print(tokens[:30])\n",
        "print(\"\\n\" + \"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGNRPPxP7izG",
        "outputId": "1626ba73-7116-461f-ea27-0ed96d088342"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 30 tokens:\n",
            "['thiss', 'is', 'a', 'samplle', 'textt', 'corpus', 'for', 'nlp', 'pre', 'processing', '.', 'it', 'containss', 'spellling', 'errorrs', 'and', 'runn-onn', 'sentencess', '.', 'the', 'weather', 'is', 'nicce', 'todayy', ',', 'isnt', 'it', '?', 'we', 'went']\n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct spelling for each token\n",
        "corrected_tokens = []\n",
        "for token in tokens:\n",
        "    blob = TextBlob(token)\n",
        "    corrected_token = str(blob.correct())\n",
        "    corrected_tokens.append(corrected_token)\n",
        "\n",
        "# Print first 10 corrected tokens\n",
        "print(\"First 10 corrected tokens:\")\n",
        "print(corrected_tokens[:10])\n",
        "\n",
        "# Reconstruct corrected text corpus (join tokens with spaces, add basic punctuation back if needed)\n",
        "corrected_text = ' '.join(corrected_tokens)\n",
        "print(\"\\nCorrected text corpus:\")\n",
        "print(corrected_text)\n",
        "print(\"\\n\" + \"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWmSz1fW7kiI",
        "outputId": "b70fd019-919c-4c0c-e704-d5787c871b4e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 corrected tokens:\n",
            "['this', 'is', 'a', 'sample', 'text', 'corpus', 'for', 'nap', 'pre', 'processing']\n",
            "\n",
            "Corrected text corpus:\n",
            "this is a sample text corpus for nap pre processing . it contains spelling errors and run-on sentences . the weather is nice today , isn it ? we went to the park but it raised heavily . machine learning is funny but challenging .\n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply POS tags to corrected tokens\n",
        "pos_tags = pos_tag(corrected_tokens)\n",
        "\n",
        "print(\"POS tags for corrected tokens:\")\n",
        "for token, pos in pos_tags:\n",
        "    print(f\"{token}: {pos}\")\n",
        "print(\"\\n\" + \"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDyTg2am7nGY",
        "outputId": "609a0558-ce97-4e4f-9b5f-b59c18b4e501"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tags for corrected tokens:\n",
            "this: DT\n",
            "is: VBZ\n",
            "a: DT\n",
            "sample: JJ\n",
            "text: NN\n",
            "corpus: NN\n",
            "for: IN\n",
            "nap: JJ\n",
            "pre: NN\n",
            "processing: NN\n",
            ".: .\n",
            "it: PRP\n",
            "contains: VBZ\n",
            "spelling: VBG\n",
            "errors: NNS\n",
            "and: CC\n",
            "run-on: JJ\n",
            "sentences: NNS\n",
            ".: .\n",
            "the: DT\n",
            "weather: NN\n",
            "is: VBZ\n",
            "nice: JJ\n",
            "today: NN\n",
            ",: ,\n",
            "isn: VB\n",
            "it: PRP\n",
            "?: .\n",
            "we: PRP\n",
            "went: VBD\n",
            "to: TO\n",
            "the: DT\n",
            "park: NN\n",
            "but: CC\n",
            "it: PRP\n",
            "raised: VBD\n",
            "heavily: RB\n",
            ".: .\n",
            "machine: NN\n",
            "learning: NN\n",
            "is: VBZ\n",
            "funny: JJ\n",
            "but: CC\n",
            "challenging: VBG\n",
            ".: .\n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words\n",
        "filtered_tokens = [token for token in corrected_tokens if token not in stop_words and len(token) > 2]\n",
        "\n",
        "print(\"First 20 tokens after removing stop words:\")\n",
        "print(filtered_tokens[:20])\n",
        "print(\"\\n\" + \"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq-d_Ae87pKA",
        "outputId": "abadca9b-c0a0-4d2c-97ab-083690356cf9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 tokens after removing stop words:\n",
            "['sample', 'text', 'corpus', 'nap', 'pre', 'processing', 'contains', 'spelling', 'errors', 'run-on', 'sentences', 'weather', 'nice', 'today', 'went', 'park', 'raised', 'heavily', 'machine', 'learning']\n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "stemmed_tokens = [ps.stem(token) for token in corrected_tokens]\n",
        "\n",
        "# Lemmatization (use POS for better accuracy, but simple version here)\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in corrected_tokens]\n",
        "\n",
        "print(\"First 20 stemmed tokens:\")\n",
        "print(stemmed_tokens[:20])\n",
        "\n",
        "print(\"\\nFirst 20 lemmatized tokens:\")\n",
        "print(lemmatized_tokens[:20])\n",
        "print(\"\\n\" + \"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAfvrV9h7rNU",
        "outputId": "ea27e52e-08b5-423e-f58a-be66943817b3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 stemmed tokens:\n",
            "['thi', 'is', 'a', 'sampl', 'text', 'corpu', 'for', 'nap', 'pre', 'process', '.', 'it', 'contain', 'spell', 'error', 'and', 'run-on', 'sentenc', '.', 'the']\n",
            "\n",
            "First 20 lemmatized tokens:\n",
            "['this', 'is', 'a', 'sample', 'text', 'corpus', 'for', 'nap', 'pre', 'processing', '.', 'it', 'contains', 'spelling', 'error', 'and', 'run-on', 'sentence', '.', 'the']\n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect sentence boundaries on raw (or corrected) text\n",
        "sentences = sent_tokenize(raw_text)\n",
        "num_sentences = len(sentences)\n",
        "\n",
        "print(f\"Total number of sentences: {num_sentences}\")\n",
        "print(\"\\nSample sentences:\")\n",
        "for i, sent in enumerate(sentences[:3]):  # Print first 3\n",
        "    print(f\"{i+1}: {sent}\")\n",
        "print(\"\\n\" + \"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_imDMEO7tZG",
        "outputId": "ce34c78f-5779-46e5-91ed-0b5260ea380a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of sentences: 5\n",
            "\n",
            "Sample sentences:\n",
            "1: Thiss is a samplle textt corpus for NLP pre processing.\n",
            "2: It containss spellling errorrs and runn-onn sentencess.\n",
            "3: The weather is nicce todayy, isnt it?\n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}
