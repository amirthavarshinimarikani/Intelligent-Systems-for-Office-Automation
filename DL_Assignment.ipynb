{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZDt6VGRPK2l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccc1dcbb-777d-4d65-e98d-506b28f2c614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "\u001b[1m169001437/169001437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n",
            "Image size: 64 X 64 = 4096\n",
            "Patch size: 8 X 8 = 64 \n",
            "Patches per image: 64\n",
            "Elements per patch (3 channels): 192\n",
            "Epoch 1/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 52ms/step - acc: 0.0744 - loss: 4.1608 - top5-acc: 0.2370 - val_acc: 0.1878 - val_loss: 3.4588 - val_top5-acc: 0.4462 - learning_rate: 0.0050\n",
            "Epoch 2/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - acc: 0.1848 - loss: 3.4210 - top5-acc: 0.4510 - val_acc: 0.2104 - val_loss: 3.2749 - val_top5-acc: 0.5090 - learning_rate: 0.0050\n",
            "Epoch 3/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - acc: 0.2183 - loss: 3.2155 - top5-acc: 0.5004 - val_acc: 0.2576 - val_loss: 3.0412 - val_top5-acc: 0.5576 - learning_rate: 0.0050\n",
            "Epoch 4/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - acc: 0.2489 - loss: 3.0455 - top5-acc: 0.5458 - val_acc: 0.2532 - val_loss: 3.0424 - val_top5-acc: 0.5650 - learning_rate: 0.0050\n",
            "Epoch 5/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - acc: 0.2740 - loss: 2.9248 - top5-acc: 0.5768 - val_acc: 0.3026 - val_loss: 2.8340 - val_top5-acc: 0.6110 - learning_rate: 0.0050\n",
            "Epoch 6/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - acc: 0.2903 - loss: 2.8335 - top5-acc: 0.6023 - val_acc: 0.3046 - val_loss: 2.7674 - val_top5-acc: 0.6184 - learning_rate: 0.0050\n",
            "Epoch 7/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - acc: 0.3085 - loss: 2.7546 - top5-acc: 0.6186 - val_acc: 0.3306 - val_loss: 2.7003 - val_top5-acc: 0.6388 - learning_rate: 0.0050\n",
            "Epoch 8/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 50ms/step - acc: 0.3165 - loss: 2.7083 - top5-acc: 0.6319 - val_acc: 0.3286 - val_loss: 2.6650 - val_top5-acc: 0.6486 - learning_rate: 0.0050\n",
            "Epoch 9/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 51ms/step - acc: 0.3265 - loss: 2.6656 - top5-acc: 0.6362 - val_acc: 0.3394 - val_loss: 2.6758 - val_top5-acc: 0.6422 - learning_rate: 0.0050\n",
            "Epoch 10/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 51ms/step - acc: 0.3412 - loss: 2.5992 - top5-acc: 0.6552 - val_acc: 0.3488 - val_loss: 2.6151 - val_top5-acc: 0.6672 - learning_rate: 0.0050\n",
            "Epoch 11/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 51ms/step - acc: 0.3441 - loss: 2.5776 - top5-acc: 0.6626 - val_acc: 0.3520 - val_loss: 2.6206 - val_top5-acc: 0.6696 - learning_rate: 0.0050\n",
            "Epoch 12/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - acc: 0.3482 - loss: 2.5484 - top5-acc: 0.6629 - val_acc: 0.3506 - val_loss: 2.6595 - val_top5-acc: 0.6636 - learning_rate: 0.0050\n",
            "Epoch 13/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 51ms/step - acc: 0.3550 - loss: 2.5242 - top5-acc: 0.6713 - val_acc: 0.3584 - val_loss: 2.5837 - val_top5-acc: 0.6680 - learning_rate: 0.0050\n",
            "Epoch 14/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 50ms/step - acc: 0.3610 - loss: 2.4790 - top5-acc: 0.6813 - val_acc: 0.3598 - val_loss: 2.5997 - val_top5-acc: 0.6686 - learning_rate: 0.0050\n",
            "Epoch 15/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - acc: 0.3642 - loss: 2.4612 - top5-acc: 0.6819 - val_acc: 0.3680 - val_loss: 2.5670 - val_top5-acc: 0.6690 - learning_rate: 0.0050\n",
            "Epoch 16/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - acc: 0.3691 - loss: 2.4565 - top5-acc: 0.6848 - val_acc: 0.3726 - val_loss: 2.5018 - val_top5-acc: 0.6906 - learning_rate: 0.0050\n",
            "Epoch 17/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 51ms/step - acc: 0.3754 - loss: 2.4194 - top5-acc: 0.6923 - val_acc: 0.3776 - val_loss: 2.4958 - val_top5-acc: 0.6922 - learning_rate: 0.0050\n",
            "Epoch 18/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - acc: 0.3766 - loss: 2.4154 - top5-acc: 0.6934 - val_acc: 0.3656 - val_loss: 2.5951 - val_top5-acc: 0.6726 - learning_rate: 0.0050\n",
            "Epoch 19/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - acc: 0.3833 - loss: 2.3825 - top5-acc: 0.7013 - val_acc: 0.3778 - val_loss: 2.5199 - val_top5-acc: 0.6934 - learning_rate: 0.0050\n",
            "Epoch 20/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 51ms/step - acc: 0.3799 - loss: 2.3888 - top5-acc: 0.7010 - val_acc: 0.3836 - val_loss: 2.4746 - val_top5-acc: 0.6948 - learning_rate: 0.0050\n",
            "Epoch 21/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - acc: 0.3927 - loss: 2.3444 - top5-acc: 0.7115 - val_acc: 0.3812 - val_loss: 2.5535 - val_top5-acc: 0.6824 - learning_rate: 0.0050\n",
            "Epoch 22/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 51ms/step - acc: 0.3898 - loss: 2.3548 - top5-acc: 0.7072 - val_acc: 0.3920 - val_loss: 2.4727 - val_top5-acc: 0.7052 - learning_rate: 0.0050\n",
            "Epoch 23/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - acc: 0.3929 - loss: 2.3403 - top5-acc: 0.7060 - val_acc: 0.3918 - val_loss: 2.5098 - val_top5-acc: 0.7034 - learning_rate: 0.0050\n",
            "Epoch 24/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 52ms/step - acc: 0.4013 - loss: 2.3150 - top5-acc: 0.7161 - val_acc: 0.4008 - val_loss: 2.4163 - val_top5-acc: 0.7152 - learning_rate: 0.0050\n",
            "Epoch 25/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - acc: 0.4012 - loss: 2.2892 - top5-acc: 0.7204 - val_acc: 0.4046 - val_loss: 2.4145 - val_top5-acc: 0.7182 - learning_rate: 0.0050\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - acc: 0.4153 - loss: 2.3600 - top5-acc: 0.7237\n",
            "Test accuracy: 41.33%\n",
            "Test top 5 accuracy: 71.75%\n",
            "Epoch 1/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - acc: 0.0479 - loss: 4.3415 - top5-acc: 0.1651 - val_acc: 0.1186 - val_loss: 3.7506 - val_top5-acc: 0.3452 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - acc: 0.1367 - loss: 3.6598 - top5-acc: 0.3698 - val_acc: 0.1986 - val_loss: 3.2929 - val_top5-acc: 0.4766 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - acc: 0.2039 - loss: 3.2874 - top5-acc: 0.4790 - val_acc: 0.2542 - val_loss: 3.0021 - val_top5-acc: 0.5478 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - acc: 0.2501 - loss: 3.0197 - top5-acc: 0.5471 - val_acc: 0.2850 - val_loss: 2.8186 - val_top5-acc: 0.5976 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - acc: 0.2805 - loss: 2.8562 - top5-acc: 0.5887 - val_acc: 0.3138 - val_loss: 2.7164 - val_top5-acc: 0.6200 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - acc: 0.3006 - loss: 2.7527 - top5-acc: 0.6135 - val_acc: 0.3170 - val_loss: 2.6734 - val_top5-acc: 0.6312 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - acc: 0.3202 - loss: 2.6511 - top5-acc: 0.6379 - val_acc: 0.3444 - val_loss: 2.5670 - val_top5-acc: 0.6574 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - acc: 0.3397 - loss: 2.5616 - top5-acc: 0.6584 - val_acc: 0.3450 - val_loss: 2.5511 - val_top5-acc: 0.6586 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 56ms/step - acc: 0.3551 - loss: 2.5024 - top5-acc: 0.6721 - val_acc: 0.3648 - val_loss: 2.4552 - val_top5-acc: 0.6800 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - acc: 0.3661 - loss: 2.4311 - top5-acc: 0.6880 - val_acc: 0.3710 - val_loss: 2.4441 - val_top5-acc: 0.6890 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - acc: 0.3768 - loss: 2.3655 - top5-acc: 0.7016 - val_acc: 0.3758 - val_loss: 2.4354 - val_top5-acc: 0.6862 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - acc: 0.3851 - loss: 2.3478 - top5-acc: 0.7059 - val_acc: 0.3878 - val_loss: 2.3627 - val_top5-acc: 0.7062 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - acc: 0.3965 - loss: 2.2992 - top5-acc: 0.7158 - val_acc: 0.4014 - val_loss: 2.3257 - val_top5-acc: 0.7052 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - acc: 0.3993 - loss: 2.2689 - top5-acc: 0.7202 - val_acc: 0.3958 - val_loss: 2.3258 - val_top5-acc: 0.7082 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - acc: 0.4085 - loss: 2.2353 - top5-acc: 0.7278 - val_acc: 0.4048 - val_loss: 2.3063 - val_top5-acc: 0.7192 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - acc: 0.4145 - loss: 2.1979 - top5-acc: 0.7372 - val_acc: 0.4130 - val_loss: 2.2745 - val_top5-acc: 0.7200 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 56ms/step - acc: 0.4253 - loss: 2.1566 - top5-acc: 0.7435 - val_acc: 0.4028 - val_loss: 2.3214 - val_top5-acc: 0.7092 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - acc: 0.4290 - loss: 2.1306 - top5-acc: 0.7475 - val_acc: 0.4184 - val_loss: 2.2557 - val_top5-acc: 0.7282 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - acc: 0.4338 - loss: 2.1052 - top5-acc: 0.7595 - val_acc: 0.4204 - val_loss: 2.2525 - val_top5-acc: 0.7292 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - acc: 0.4471 - loss: 2.0548 - top5-acc: 0.7670 - val_acc: 0.4204 - val_loss: 2.2198 - val_top5-acc: 0.7348 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - acc: 0.4502 - loss: 2.0203 - top5-acc: 0.7731 - val_acc: 0.4148 - val_loss: 2.2315 - val_top5-acc: 0.7372 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - acc: 0.4605 - loss: 2.0133 - top5-acc: 0.7743 - val_acc: 0.4206 - val_loss: 2.2135 - val_top5-acc: 0.7358 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - acc: 0.4614 - loss: 1.9842 - top5-acc: 0.7813 - val_acc: 0.4250 - val_loss: 2.2026 - val_top5-acc: 0.7370 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - acc: 0.4617 - loss: 1.9686 - top5-acc: 0.7832 - val_acc: 0.4294 - val_loss: 2.2105 - val_top5-acc: 0.7348 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - acc: 0.4734 - loss: 1.9408 - top5-acc: 0.7901 - val_acc: 0.4368 - val_loss: 2.1891 - val_top5-acc: 0.7422 - learning_rate: 0.0010\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - acc: 0.4455 - loss: 2.1604 - top5-acc: 0.7481\n",
            "Test accuracy: 44.15%\n",
            "Test top 5 accuracy: 74.67%\n",
            "Epoch 1/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 68ms/step - acc: 0.0681 - loss: 4.1825 - top5-acc: 0.2224 - val_acc: 0.1590 - val_loss: 3.5234 - val_top5-acc: 0.4178 - learning_rate: 0.0030\n",
            "Epoch 2/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 67ms/step - acc: 0.1622 - loss: 3.4931 - top5-acc: 0.4221 - val_acc: 0.2096 - val_loss: 3.2737 - val_top5-acc: 0.4936 - learning_rate: 0.0030\n",
            "Epoch 3/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 74ms/step - acc: 0.2063 - loss: 3.2678 - top5-acc: 0.4835 - val_acc: 0.2348 - val_loss: 3.1561 - val_top5-acc: 0.5188 - learning_rate: 0.0030\n",
            "Epoch 4/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 67ms/step - acc: 0.2300 - loss: 3.1322 - top5-acc: 0.5213 - val_acc: 0.2480 - val_loss: 3.0524 - val_top5-acc: 0.5450 - learning_rate: 0.0030\n",
            "Epoch 5/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 67ms/step - acc: 0.2573 - loss: 2.9965 - top5-acc: 0.5566 - val_acc: 0.2708 - val_loss: 2.9063 - val_top5-acc: 0.5816 - learning_rate: 0.0030\n",
            "Epoch 6/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 67ms/step - acc: 0.2763 - loss: 2.8826 - top5-acc: 0.5816 - val_acc: 0.2862 - val_loss: 2.8250 - val_top5-acc: 0.6006 - learning_rate: 0.0030\n",
            "Epoch 7/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 66ms/step - acc: 0.3000 - loss: 2.7645 - top5-acc: 0.6120 - val_acc: 0.3032 - val_loss: 2.7528 - val_top5-acc: 0.6230 - learning_rate: 0.0030\n",
            "Epoch 8/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 66ms/step - acc: 0.3086 - loss: 2.7111 - top5-acc: 0.6211 - val_acc: 0.3160 - val_loss: 2.7050 - val_top5-acc: 0.6296 - learning_rate: 0.0030\n",
            "Epoch 9/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 67ms/step - acc: 0.3195 - loss: 2.6598 - top5-acc: 0.6355 - val_acc: 0.3272 - val_loss: 2.6630 - val_top5-acc: 0.6404 - learning_rate: 0.0030\n",
            "Epoch 10/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 67ms/step - acc: 0.3361 - loss: 2.5969 - top5-acc: 0.6505 - val_acc: 0.3428 - val_loss: 2.6079 - val_top5-acc: 0.6466 - learning_rate: 0.0030\n",
            "Epoch 11/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 66ms/step - acc: 0.3513 - loss: 2.5388 - top5-acc: 0.6625 - val_acc: 0.3478 - val_loss: 2.5827 - val_top5-acc: 0.6614 - learning_rate: 0.0030\n",
            "Epoch 12/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 67ms/step - acc: 0.3701 - loss: 2.4386 - top5-acc: 0.6852 - val_acc: 0.3716 - val_loss: 2.4513 - val_top5-acc: 0.6862 - learning_rate: 0.0030\n",
            "Epoch 13/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 66ms/step - acc: 0.3771 - loss: 2.3879 - top5-acc: 0.6951 - val_acc: 0.3722 - val_loss: 2.4391 - val_top5-acc: 0.6814 - learning_rate: 0.0030\n",
            "Epoch 14/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 67ms/step - acc: 0.3874 - loss: 2.3351 - top5-acc: 0.7060 - val_acc: 0.3860 - val_loss: 2.3943 - val_top5-acc: 0.7014 - learning_rate: 0.0030\n",
            "Epoch 15/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 67ms/step - acc: 0.3965 - loss: 2.3004 - top5-acc: 0.7118 - val_acc: 0.3842 - val_loss: 2.4030 - val_top5-acc: 0.7022 - learning_rate: 0.0030\n",
            "Epoch 16/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 67ms/step - acc: 0.4090 - loss: 2.2492 - top5-acc: 0.7240 - val_acc: 0.3900 - val_loss: 2.3985 - val_top5-acc: 0.7104 - learning_rate: 0.0030\n",
            "Epoch 17/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 67ms/step - acc: 0.4105 - loss: 2.2196 - top5-acc: 0.7315 - val_acc: 0.3964 - val_loss: 2.3536 - val_top5-acc: 0.7186 - learning_rate: 0.0030\n",
            "Epoch 18/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 67ms/step - acc: 0.4228 - loss: 2.1972 - top5-acc: 0.7331 - val_acc: 0.3990 - val_loss: 2.3515 - val_top5-acc: 0.7068 - learning_rate: 0.0030\n",
            "Epoch 19/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 67ms/step - acc: 0.4283 - loss: 2.1592 - top5-acc: 0.7421 - val_acc: 0.3934 - val_loss: 2.3746 - val_top5-acc: 0.7096 - learning_rate: 0.0030\n",
            "Epoch 20/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 67ms/step - acc: 0.4314 - loss: 2.1468 - top5-acc: 0.7467 - val_acc: 0.4118 - val_loss: 2.3120 - val_top5-acc: 0.7174 - learning_rate: 0.0030\n",
            "Epoch 21/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 67ms/step - acc: 0.4366 - loss: 2.1196 - top5-acc: 0.7500 - val_acc: 0.4124 - val_loss: 2.3180 - val_top5-acc: 0.7252 - learning_rate: 0.0030\n",
            "Epoch 22/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 67ms/step - acc: 0.4409 - loss: 2.0850 - top5-acc: 0.7575 - val_acc: 0.4210 - val_loss: 2.2905 - val_top5-acc: 0.7234 - learning_rate: 0.0030\n",
            "Epoch 23/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 66ms/step - acc: 0.4461 - loss: 2.0854 - top5-acc: 0.7567 - val_acc: 0.4162 - val_loss: 2.3202 - val_top5-acc: 0.7238 - learning_rate: 0.0030\n",
            "Epoch 24/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 66ms/step - acc: 0.4491 - loss: 2.0607 - top5-acc: 0.7635 - val_acc: 0.4148 - val_loss: 2.3271 - val_top5-acc: 0.7200 - learning_rate: 0.0030\n",
            "Epoch 25/25\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 67ms/step - acc: 0.4549 - loss: 2.0262 - top5-acc: 0.7716 - val_acc: 0.4184 - val_loss: 2.3295 - val_top5-acc: 0.7160 - learning_rate: 0.0030\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - acc: 0.4215 - loss: 2.2681 - top5-acc: 0.7255\n",
            "Test accuracy: 42.23%\n",
            "Test top 5 accuracy: 72.72%\n"
          ]
        }
      ],
      "source": [
        "#Assignment 2 - Deep Learning\n",
        "# 1.Image classification with modern MLP models\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "# Prepare the data\n",
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Configure the hyperparameters\n",
        "weight_decay = 0.0001\n",
        "batch_size = 128\n",
        "num_epochs = 25  # Set to 25 epochs\n",
        "dropout_rate = 0.2\n",
        "image_size = 64  # We'll resize input images to this size.\n",
        "patch_size = 8  # Size of the patches to be extracted from the input images.\n",
        "num_patches = (image_size // patch_size) ** 2  # Size of the data array.\n",
        "embedding_dim = 256  # Number of hidden units.\n",
        "num_blocks = 4  # Number of blocks.\n",
        "\n",
        "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
        "print(f\"Patches per image: {num_patches}\")\n",
        "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
        "\n",
        "# Use data augmentation\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)\n",
        "\n",
        "# Implement patch extraction as a layer\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, x):\n",
        "        patches = keras.ops.image.extract_patches(x, self.patch_size)\n",
        "        batch_size = keras.ops.shape(patches)[0]\n",
        "        num_patches = keras.ops.shape(patches)[1] * keras.ops.shape(patches)[2]\n",
        "        patch_dim = keras.ops.shape(patches)[3]\n",
        "        out = keras.ops.reshape(patches, (batch_size, num_patches, patch_dim))\n",
        "        return out\n",
        "\n",
        "# Implement position embedding as a layer\n",
        "class PositionEmbedding(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        sequence_length,\n",
        "        initializer=\"glorot_uniform\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        if sequence_length is None:\n",
        "            raise ValueError(\"`sequence_length` must be an Integer, received `None`.\")\n",
        "        self.sequence_length = int(sequence_length)\n",
        "        self.initializer = keras.initializers.get(initializer)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"sequence_length\": self.sequence_length,\n",
        "                \"initializer\": keras.initializers.serialize(self.initializer),\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        feature_size = input_shape[-1]\n",
        "        self.position_embeddings = self.add_weight(\n",
        "            name=\"embeddings\",\n",
        "            shape=[self.sequence_length, feature_size],\n",
        "            initializer=self.initializer,\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, start_index=0):\n",
        "        shape = keras.ops.shape(inputs)\n",
        "        feature_length = shape[-1]\n",
        "        sequence_length = shape[-2]\n",
        "        # trim to match the length of the input sequence, which might be less\n",
        "        # than the sequence_length of the layer.\n",
        "        position_embeddings = keras.ops.convert_to_tensor(self.position_embeddings)\n",
        "        position_embeddings = keras.ops.slice(\n",
        "            position_embeddings,\n",
        "            (start_index, 0),\n",
        "            (sequence_length, feature_length),\n",
        "        )\n",
        "        return keras.ops.broadcast_to(position_embeddings, shape)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "# Build a classification model\n",
        "def build_classifier(blocks, positional_encoding=False):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
        "    x = layers.Dense(units=embedding_dim)(patches)\n",
        "    if positional_encoding:\n",
        "        x = x + PositionEmbedding(sequence_length=num_patches)(x)\n",
        "    # Process x using the module blocks.\n",
        "    x = blocks(x)\n",
        "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor.\n",
        "    representation = layers.GlobalAveragePooling1D()(x)\n",
        "    # Apply dropout.\n",
        "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
        "    # Compute logits outputs.\n",
        "    logits = layers.Dense(num_classes)(representation)\n",
        "    # Create the Keras model.\n",
        "    return keras.Model(inputs=inputs, outputs=logits)\n",
        "\n",
        "# Define an experiment\n",
        "def run_experiment(model):\n",
        "    # Create Adam optimizer with weight decay.\n",
        "    optimizer = keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=weight_decay,\n",
        "    )\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
        "        ],\n",
        "    )\n",
        "    # Create a learning rate scheduler callback.\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=5\n",
        "    )\n",
        "    # Create an early stopping callback.\n",
        "    early_stopping = keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
        "    )\n",
        "    # Fit the model with verbose output to monitor progress\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        verbose=1,  # Show progress during training\n",
        "    )\n",
        "\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    # Return history to plot learning curves.\n",
        "    return history\n",
        "\n",
        "# The MLP-Mixer model\n",
        "class MLPMixerLayer(layers.Layer):\n",
        "    def __init__(self, num_patches, hidden_units, dropout_rate, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.mlp1 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=num_patches, activation=\"gelu\"),\n",
        "                layers.Dense(units=num_patches),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "        self.mlp2 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=num_patches, activation=\"gelu\"),\n",
        "                layers.Dense(units=hidden_units),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        return super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply layer normalization.\n",
        "        x = self.normalize(inputs)\n",
        "        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
        "        x_channels = keras.ops.transpose(x, axes=(0, 2, 1))\n",
        "        # Apply mlp1 on each channel independently.\n",
        "        mlp1_outputs = self.mlp1(x_channels)\n",
        "        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
        "        mlp1_outputs = keras.ops.transpose(mlp1_outputs, axes=(0, 2, 1))\n",
        "        # Add skip connection.\n",
        "        x = mlp1_outputs + inputs\n",
        "        # Apply layer normalization.\n",
        "        x_patches = self.normalize(x)\n",
        "        # Apply mlp2 on each patch independently.\n",
        "        mlp2_outputs = self.mlp2(x_patches)\n",
        "        # Add skip connection.\n",
        "        x = x + mlp2_outputs\n",
        "        return x\n",
        "\n",
        "# Build, train, and evaluate the MLP-Mixer model\n",
        "mlpmixer_blocks = keras.Sequential(\n",
        "    [MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",
        ")\n",
        "learning_rate = 0.005\n",
        "mlpmixer_classifier = build_classifier(mlpmixer_blocks)\n",
        "history = run_experiment(mlpmixer_classifier)\n",
        "# Save with .keras format\n",
        "mlpmixer_classifier.save('mlpmixer_model.keras')\n",
        "\n",
        "# The FNet model\n",
        "class FNetLayer(layers.Layer):\n",
        "    def __init__(self, embedding_dim, dropout_rate, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embedding_dim, activation=\"gelu\"),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "                layers.Dense(units=embedding_dim),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply fourier transformations.\n",
        "        real_part = inputs\n",
        "        im_part = keras.ops.zeros_like(inputs)\n",
        "        x = keras.ops.fft2((real_part, im_part))[0]\n",
        "        # Add skip connection.\n",
        "        x = x + inputs\n",
        "        # Apply layer normalization.\n",
        "        x = self.normalize1(x)\n",
        "        # Apply Feedforward network.\n",
        "        x_ffn = self.ffn(x)\n",
        "        # Add skip connection.\n",
        "        x = x + x_ffn\n",
        "        # Apply layer normalization.\n",
        "        return self.normalize2(x)\n",
        "\n",
        "# Build, train, and evaluate the FNet model\n",
        "fnet_blocks = keras.Sequential(\n",
        "    [FNetLayer(embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",
        ")\n",
        "learning_rate = 0.001\n",
        "fnet_classifier = build_classifier(fnet_blocks, positional_encoding=True)\n",
        "history = run_experiment(fnet_classifier)\n",
        "# Save with .keras format\n",
        "fnet_classifier.save('fnet_model.keras')\n",
        "\n",
        "# The gMLP model\n",
        "class gMLPLayer(layers.Layer):\n",
        "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.channel_projection1 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embedding_dim * 2, activation=\"gelu\"),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.channel_projection2 = layers.Dense(units=embedding_dim)\n",
        "\n",
        "        self.spatial_projection = layers.Dense(\n",
        "            units=num_patches, bias_initializer=\"Ones\"\n",
        "        )\n",
        "\n",
        "        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def spatial_gating_unit(self, x):\n",
        "        # Split x along the channel dimensions.\n",
        "        # Tensors u and v will in the shape of [batch_size, num_patchs, embedding_dim].\n",
        "        u, v = keras.ops.split(x, indices_or_sections=2, axis=2)\n",
        "        # Apply layer normalization.\n",
        "        v = self.normalize2(v)\n",
        "        # Apply spatial projection.\n",
        "        v_channels = keras.ops.transpose(v, axes=(0, 2, 1))\n",
        "        v_projected = self.spatial_projection(v_channels)\n",
        "        v_projected = keras.ops.transpose(v_projected, axes=(0, 2, 1))\n",
        "        # Apply element-wise multiplication.\n",
        "        return u * v_projected\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply layer normalization.\n",
        "        x = self.normalize1(inputs)\n",
        "        # Apply the first channel projection. x_projected shape: [batch_size, num_patches, embedding_dim * 2].\n",
        "        x_projected = self.channel_projection1(x)\n",
        "        # Apply the spatial gating unit. x_spatial shape: [batch_size, num_patches, embedding_dim].\n",
        "        x_spatial = self.spatial_gating_unit(x_projected)\n",
        "        # Apply the second channel projection. x_projected shape: [batch_size, num_patches, embedding_dim].\n",
        "        x_projected = self.channel_projection2(x_spatial)\n",
        "        # Add skip connection.\n",
        "        return x + x_projected\n",
        "\n",
        "# Build, train, and evaluate the gMLP model\n",
        "gmlp_blocks = keras.Sequential(\n",
        "    [gMLPLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",
        ")\n",
        "learning_rate = 0.003\n",
        "gmlp_classifier = build_classifier(gmlp_blocks)\n",
        "history = run_experiment(gmlp_classifier)\n",
        "# Save with .keras format\n",
        "gmlp_classifier.save('gmlp_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) mplement RNN for sentiment analysis on movie reviews.\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# Load IMDB dataset\n",
        "max_features = 10000  # Top 10,000 words\n",
        "max_len = 500  # Max sequence length\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "x_train = pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "# Build LSTM model (updated to remove input_length)\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))  # Removed input_length\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "model.save('sentiment_rnn.keras')"
      ],
      "metadata": {
        "id": "OBMsvy3xPRA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "958e55e1-acfa-4ff2-8c90-c9b1f89e139e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m631s\u001b[0m 1s/step - accuracy: 0.6961 - loss: 0.5582 - val_accuracy: 0.8344 - val_loss: 0.3810\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m626s\u001b[0m 1s/step - accuracy: 0.8355 - loss: 0.3805 - val_accuracy: 0.8200 - val_loss: 0.4229\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m628s\u001b[0m 1s/step - accuracy: 0.8708 - loss: 0.3102 - val_accuracy: 0.8470 - val_loss: 0.3652\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m628s\u001b[0m 1s/step - accuracy: 0.9118 - loss: 0.2347 - val_accuracy: 0.8602 - val_loss: 0.3532\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m628s\u001b[0m 1s/step - accuracy: 0.9352 - loss: 0.1774 - val_accuracy: 0.7990 - val_loss: 0.4767\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 285ms/step - accuracy: 0.7868 - loss: 0.5082\n",
            "Accuracy: 78.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.Demonstrate the different types of autoencoders using Fashion MNIST dataset and any industrial dataset.\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# constants\n",
        "NUM_EPOCHS = 50\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "trainset = datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "testset = datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "trainloader = DataLoader(\n",
        "    trainset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "testloader = DataLoader(\n",
        "    testset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# utility functions\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = 'cuda:0'\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "    return device\n",
        "\n",
        "def make_dir():\n",
        "    image_dir = 'FashionMNIST_Images'\n",
        "    if not os.path.exists(image_dir):\n",
        "        os.makedirs(image_dir)\n",
        "\n",
        "def save_decoded_image(img, epoch):\n",
        "    img = img.view(img.size(0), 1, 28, 28)\n",
        "    save_image(img, './FashionMNIST_Images/linear_ae_image{}.png'.format(epoch))\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        # encoder\n",
        "        self.enc1 = nn.Linear(in_features=784, out_features=256)\n",
        "        self.enc2 = nn.Linear(in_features=256, out_features=128)\n",
        "        self.enc3 = nn.Linear(in_features=128, out_features=64)\n",
        "        self.enc4 = nn.Linear(in_features=64, out_features=32)\n",
        "        self.enc5 = nn.Linear(in_features=32, out_features=16)\n",
        "\n",
        "        # decoder\n",
        "        self.dec1 = nn.Linear(in_features=16, out_features=32)\n",
        "        self.dec2 = nn.Linear(in_features=32, out_features=64)\n",
        "        self.dec3 = nn.Linear(in_features=64, out_features=128)\n",
        "        self.dec4 = nn.Linear(in_features=128, out_features=256)\n",
        "        self.dec5 = nn.Linear(in_features=256, out_features=784)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.enc1(x))\n",
        "        x = F.relu(self.enc2(x))\n",
        "        x = F.relu(self.enc3(x))\n",
        "        x = F.relu(self.enc4(x))\n",
        "        x = F.relu(self.enc5(x))\n",
        "\n",
        "        x = F.relu(self.dec1(x))\n",
        "        x = F.relu(self.dec2(x))\n",
        "        x = F.relu(self.dec3(x))\n",
        "        x = F.relu(self.dec4(x))\n",
        "        x = F.relu(self.dec5(x))\n",
        "        return x\n",
        "\n",
        "net = Autoencoder()\n",
        "print(net)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "def train(net, trainloader, NUM_EPOCHS):\n",
        "    train_loss = []\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        running_loss = 0.0\n",
        "        for data in trainloader:\n",
        "            img, _ = data\n",
        "            img = img.to(device)\n",
        "            img = img.view(img.size(0), -1)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(img)\n",
        "            loss = criterion(outputs, img)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        loss = running_loss / len(trainloader)\n",
        "        train_loss.append(loss)\n",
        "        print('Epoch {} of {}, Train Loss: {:.3f}'.format(\n",
        "            epoch+1, NUM_EPOCHS, loss))\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            save_decoded_image(outputs.cpu().data, epoch)\n",
        "\n",
        "    return train_loss\n",
        "\n",
        "def test_image_reconstruction(net, testloader):\n",
        "     for batch in testloader:\n",
        "        img, _ = batch\n",
        "        img = img.to(device)\n",
        "        img = img.view(img.size(0), -1)\n",
        "        outputs = net(img)\n",
        "        outputs = outputs.view(outputs.size(0), 1, 28, 28).cpu().data\n",
        "        save_image(outputs, 'fashionmnist_reconstruction.png')\n",
        "        break\n",
        "\n",
        "# get the computation device\n",
        "device = get_device()\n",
        "print(device)\n",
        "# load the neural network onto the device\n",
        "net.to(device)\n",
        "\n",
        "make_dir()\n",
        "\n",
        "# train the network\n",
        "train_loss = train(net, trainloader, NUM_EPOCHS)\n",
        "plt.figure()\n",
        "plt.plot(train_loss)\n",
        "plt.title('Train Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.savefig('deep_ae_fashionmnist_loss.png')\n",
        "\n",
        "# test the network\n",
        "test_image_reconstruction(net, testloader)"
      ],
      "metadata": {
        "id": "N8qpUP9_PVHk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c0fd138-da4c-40b3-d3ef-91fbe5bbaefe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 12.9MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 203kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.76MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 25.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autoencoder(\n",
            "  (enc1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (enc2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (enc3): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (enc4): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (enc5): Linear(in_features=32, out_features=16, bias=True)\n",
            "  (dec1): Linear(in_features=16, out_features=32, bias=True)\n",
            "  (dec2): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (dec3): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (dec4): Linear(in_features=128, out_features=256, bias=True)\n",
            "  (dec5): Linear(in_features=256, out_features=784, bias=True)\n",
            ")\n",
            "cuda:0\n",
            "Epoch 1 of 50, Train Loss: 0.072\n",
            "Epoch 2 of 50, Train Loss: 0.038\n",
            "Epoch 3 of 50, Train Loss: 0.033\n",
            "Epoch 4 of 50, Train Loss: 0.030\n",
            "Epoch 5 of 50, Train Loss: 0.029\n",
            "Epoch 6 of 50, Train Loss: 0.028\n",
            "Epoch 7 of 50, Train Loss: 0.027\n",
            "Epoch 8 of 50, Train Loss: 0.026\n",
            "Epoch 9 of 50, Train Loss: 0.026\n",
            "Epoch 10 of 50, Train Loss: 0.025\n",
            "Epoch 11 of 50, Train Loss: 0.025\n",
            "Epoch 12 of 50, Train Loss: 0.024\n",
            "Epoch 13 of 50, Train Loss: 0.024\n",
            "Epoch 14 of 50, Train Loss: 0.023\n",
            "Epoch 15 of 50, Train Loss: 0.023\n",
            "Epoch 16 of 50, Train Loss: 0.022\n",
            "Epoch 17 of 50, Train Loss: 0.022\n",
            "Epoch 18 of 50, Train Loss: 0.022\n",
            "Epoch 19 of 50, Train Loss: 0.021\n",
            "Epoch 20 of 50, Train Loss: 0.021\n",
            "Epoch 21 of 50, Train Loss: 0.020\n",
            "Epoch 22 of 50, Train Loss: 0.020\n",
            "Epoch 23 of 50, Train Loss: 0.020\n",
            "Epoch 24 of 50, Train Loss: 0.020\n",
            "Epoch 25 of 50, Train Loss: 0.020\n",
            "Epoch 26 of 50, Train Loss: 0.019\n",
            "Epoch 27 of 50, Train Loss: 0.019\n",
            "Epoch 28 of 50, Train Loss: 0.019\n",
            "Epoch 29 of 50, Train Loss: 0.019\n",
            "Epoch 30 of 50, Train Loss: 0.019\n",
            "Epoch 31 of 50, Train Loss: 0.018\n",
            "Epoch 32 of 50, Train Loss: 0.018\n",
            "Epoch 33 of 50, Train Loss: 0.018\n",
            "Epoch 34 of 50, Train Loss: 0.018\n",
            "Epoch 35 of 50, Train Loss: 0.018\n",
            "Epoch 36 of 50, Train Loss: 0.018\n",
            "Epoch 37 of 50, Train Loss: 0.017\n",
            "Epoch 38 of 50, Train Loss: 0.017\n",
            "Epoch 39 of 50, Train Loss: 0.017\n",
            "Epoch 40 of 50, Train Loss: 0.017\n",
            "Epoch 41 of 50, Train Loss: 0.017\n",
            "Epoch 42 of 50, Train Loss: 0.017\n",
            "Epoch 43 of 50, Train Loss: 0.017\n",
            "Epoch 44 of 50, Train Loss: 0.017\n",
            "Epoch 45 of 50, Train Loss: 0.017\n",
            "Epoch 46 of 50, Train Loss: 0.017\n",
            "Epoch 47 of 50, Train Loss: 0.017\n",
            "Epoch 48 of 50, Train Loss: 0.017\n",
            "Epoch 49 of 50, Train Loss: 0.017\n",
            "Epoch 50 of 50, Train Loss: 0.017\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# Config (define your own or use these)\n",
        "class config:\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    BATCH_SIZE = 128\n",
        "    CHANNELS = 1\n",
        "    IMAGE_SIZE = 32  # Pad to 32 for even divisions\n",
        "    EMBEDDING_DIM = 64\n",
        "    LR = 0.001\n",
        "    EPOCHS = 50\n",
        "    PATIENCE = 5\n",
        "    MODEL_WEIGHTS_PATH = 'conv_ae_weights.pt'\n",
        "    training_progress_dir = 'training_progress'\n",
        "    FILE_RECON_BEFORE_TRAINING = 'recon_before.png'\n",
        "    FILE_REAL_BEFORE_TRAINING = 'real_before.png'\n",
        "    CLASS_LABELS = {0: 'T-shirt', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat', 5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}  # Fashion MNIST labels\n",
        "\n",
        "os.makedirs(config.training_progress_dir, exist_ok=True)\n",
        "\n",
        "# Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, image_size, channels, embedding_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, 32, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.shape_before_flattening = None\n",
        "        flattened_size = (image_size // 8) * (image_size // 8) * 128\n",
        "        self.fc = nn.Linear(flattened_size, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        self.shape_before_flattening = x.shape[1:]\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, shape_before_flattening, channels):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc = nn.Linear(embedding_dim, np.prod(shape_before_flattening))\n",
        "        self.reshape_dim = shape_before_flattening\n",
        "        self.deconv1 = nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.deconv3 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.conv1 = nn.Conv2d(32, channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(x.size(0), *self.reshape_dim)\n",
        "        x = F.relu(self.deconv1(x))\n",
        "        x = F.relu(self.deconv2(x))\n",
        "        x = F.relu(self.deconv3(x))\n",
        "        x = torch.sigmoid(self.conv1(x))\n",
        "        return x\n",
        "\n",
        "# Utilities (simplified display_random_images and validate)\n",
        "def display_random_images(data_loader, encoder, decoder, file_recon=None, display_real=False):\n",
        "    # Simplified: save reconstructions\n",
        "    for images, _ in data_loader:\n",
        "        images = images.to(config.DEVICE)\n",
        "        with torch.no_grad():\n",
        "            encoded = encoder(images)\n",
        "            recon = decoder(encoded)\n",
        "        save_image(recon, file_recon)\n",
        "        break\n",
        "\n",
        "def validate(encoder, decoder, test_loader, criterion):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images, _ in test_loader:\n",
        "            images = images.to(config.DEVICE)\n",
        "            encoded = encoder(images)\n",
        "            decoded = decoder(encoded)\n",
        "            loss = criterion(decoded, images)\n",
        "            val_loss += loss.item()\n",
        "    return val_loss / len(test_loader)\n",
        "\n",
        "# Training\n",
        "transform = transforms.Compose([transforms.Pad(padding=2), transforms.ToTensor()])\n",
        "trainset = datasets.FashionMNIST(\"data\", train=True, download=True, transform=transform)\n",
        "testset = datasets.FashionMNIST(\"data\", train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(trainset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(testset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
        "\n",
        "dummy_input = torch.randn(1, config.CHANNELS, config.IMAGE_SIZE, config.IMAGE_SIZE)\n",
        "encoder = Encoder(channels=config.CHANNELS, image_size=config.IMAGE_SIZE, embedding_dim=config.EMBEDDING_DIM).to(config.DEVICE)\n",
        "enc_out = encoder(dummy_input.to(config.DEVICE))\n",
        "shape_before_flattening = encoder.shape_before_flattening\n",
        "decoder = Decoder(config.EMBEDDING_DIM, shape_before_flattening, config.CHANNELS).to(config.DEVICE)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=config.LR)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=config.PATIENCE)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "for epoch in range(config.EPOCHS):\n",
        "    print(f\"Epoch: {epoch + 1}/{config.EPOCHS}\")\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, _) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        data = data.to(config.DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        encoded = encoder(data)\n",
        "        decoded = decoder(encoded)\n",
        "        loss = criterion(decoded, data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    val_loss = validate(encoder, decoder, test_loader, criterion)\n",
        "    print(f\"Epoch {epoch + 1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({\"encoder\": encoder.state_dict(), \"decoder\": decoder.state_dict()}, config.MODEL_WEIGHTS_PATH)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "    display_random_images(test_loader, encoder, decoder, file_recon=os.path.join(config.training_progress_dir, f\"epoch{epoch + 1}_test_recon.png\"))\n",
        "\n",
        "print(\"Training finished!\")"
      ],
      "metadata": {
        "id": "6KYNd9a-PfEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "356ec292-9f59-4e24-e3b1-c791a43eaa6f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:14<00:00, 33.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train Loss: 0.2697 | Val Loss: 0.2227\n",
            "Epoch: 2/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Train Loss: 0.2151 | Val Loss: 0.2125\n",
            "Epoch: 3/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Train Loss: 0.2082 | Val Loss: 0.2079\n",
            "Epoch: 4/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:14<00:00, 33.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Train Loss: 0.2048 | Val Loss: 0.2053\n",
            "Epoch: 5/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:14<00:00, 33.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Train Loss: 0.2027 | Val Loss: 0.2028\n",
            "Epoch: 6/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Train Loss: 0.2013 | Val Loss: 0.2030\n",
            "Epoch: 7/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Train Loss: 0.2003 | Val Loss: 0.2012\n",
            "Epoch: 8/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Train Loss: 0.1994 | Val Loss: 0.2005\n",
            "Epoch: 9/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 33.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 | Train Loss: 0.1987 | Val Loss: 0.2005\n",
            "Epoch: 10/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Train Loss: 0.1982 | Val Loss: 0.1995\n",
            "Epoch: 11/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 | Train Loss: 0.1976 | Val Loss: 0.1992\n",
            "Epoch: 12/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 | Train Loss: 0.1972 | Val Loss: 0.1990\n",
            "Epoch: 13/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 33.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 | Train Loss: 0.1968 | Val Loss: 0.1984\n",
            "Epoch: 14/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 35.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 | Train Loss: 0.1965 | Val Loss: 0.1989\n",
            "Epoch: 15/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 | Train Loss: 0.1962 | Val Loss: 0.1975\n",
            "Epoch: 16/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 | Train Loss: 0.1960 | Val Loss: 0.1979\n",
            "Epoch: 17/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 | Train Loss: 0.1958 | Val Loss: 0.1973\n",
            "Epoch: 18/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 | Train Loss: 0.1956 | Val Loss: 0.1975\n",
            "Epoch: 19/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 | Train Loss: 0.1954 | Val Loss: 0.1968\n",
            "Epoch: 20/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 | Train Loss: 0.1953 | Val Loss: 0.1973\n",
            "Epoch: 21/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 | Train Loss: 0.1952 | Val Loss: 0.1969\n",
            "Epoch: 22/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 | Train Loss: 0.1950 | Val Loss: 0.1975\n",
            "Epoch: 23/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 | Train Loss: 0.1949 | Val Loss: 0.1965\n",
            "Epoch: 24/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:14<00:00, 33.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 | Train Loss: 0.1948 | Val Loss: 0.1971\n",
            "Epoch: 25/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:14<00:00, 33.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 | Train Loss: 0.1947 | Val Loss: 0.1967\n",
            "Epoch: 26/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 | Train Loss: 0.1946 | Val Loss: 0.1968\n",
            "Epoch: 27/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:14<00:00, 33.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 | Train Loss: 0.1945 | Val Loss: 0.1968\n",
            "Epoch: 28/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 | Train Loss: 0.1944 | Val Loss: 0.1963\n",
            "Epoch: 29/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 33.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 | Train Loss: 0.1944 | Val Loss: 0.1966\n",
            "Epoch: 30/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 | Train Loss: 0.1943 | Val Loss: 0.1964\n",
            "Epoch: 31/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 33.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31 | Train Loss: 0.1943 | Val Loss: 0.1961\n",
            "Epoch: 32/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32 | Train Loss: 0.1942 | Val Loss: 0.1966\n",
            "Epoch: 33/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33 | Train Loss: 0.1941 | Val Loss: 0.1962\n",
            "Epoch: 34/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 33.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34 | Train Loss: 0.1941 | Val Loss: 0.1964\n",
            "Epoch: 35/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35 | Train Loss: 0.1940 | Val Loss: 0.1961\n",
            "Epoch: 36/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36 | Train Loss: 0.1940 | Val Loss: 0.1962\n",
            "Epoch: 37/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 35.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37 | Train Loss: 0.1939 | Val Loss: 0.1959\n",
            "Epoch: 38/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 33.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38 | Train Loss: 0.1939 | Val Loss: 0.1961\n",
            "Epoch: 39/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39 | Train Loss: 0.1938 | Val Loss: 0.1963\n",
            "Epoch: 40/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40 | Train Loss: 0.1938 | Val Loss: 0.1957\n",
            "Epoch: 41/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 33.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41 | Train Loss: 0.1938 | Val Loss: 0.1959\n",
            "Epoch: 42/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42 | Train Loss: 0.1937 | Val Loss: 0.1960\n",
            "Epoch: 43/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43 | Train Loss: 0.1937 | Val Loss: 0.1960\n",
            "Epoch: 44/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44 | Train Loss: 0.1937 | Val Loss: 0.1961\n",
            "Epoch: 45/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:14<00:00, 33.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45 | Train Loss: 0.1936 | Val Loss: 0.1960\n",
            "Epoch: 46/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46 | Train Loss: 0.1936 | Val Loss: 0.1958\n",
            "Epoch: 47/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47 | Train Loss: 0.1930 | Val Loss: 0.1953\n",
            "Epoch: 48/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 34.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48 | Train Loss: 0.1930 | Val Loss: 0.1957\n",
            "Epoch: 49/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 33.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49 | Train Loss: 0.1930 | Val Loss: 0.1959\n",
            "Epoch: 50/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 33.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50 | Train Loss: 0.1930 | Val Loss: 0.1954\n",
            "Training finished!\n"
          ]
        }
      ]
    }
  ]
}